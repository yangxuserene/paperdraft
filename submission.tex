\documentclass[conference]{IEEEtran}

% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
\else

\fi

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage{algorithm}
%\usepackage{algorithm2e}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{flushend}
\usepackage{url}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{graphicx}
%\usepackage{caption}
%\usepackage[labelfont=bf]{caption}
\usepackage[font=bf,labelsep=space]{caption}

\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Study of Intra/Inter-Application Interference for Smart Job Allocation on HPC system}
% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{

\IEEEauthorblockN{Xu Yang\IEEEauthorrefmark{1}, John Jenkins\IEEEauthorrefmark{2}, Misbah Mubarak\IEEEauthorrefmark{2}, Robert B. Ross\IEEEauthorrefmark{2}, Zhou Zhou\IEEEauthorrefmark{1}, Zhiling Lan\IEEEauthorrefmark{1}}

\IEEEauthorblockA{\IEEEauthorrefmark{1}Department of Computer Science,
Illinois Institute of Technology,
Chicago, Illinois, USA 60616\\
{xyang56, zzhou}@hawk.iit.edu, lan@iit.edu}

\IEEEauthorblockA{\IEEEauthorrefmark{2}Mathematics and Computer Science Division, Argonne National Laboratory,
Argonne, IL, USA 60439\\
\{jenkins,rross\}@mcs.anl.gov, mmubarak@anl.gov}
}

% make the title area
\maketitle

\begin{abstract} 

It has been widely recoginzed that network contension between concurrently running jobs in HPC system is one of the primary causes of performance variability. Optimizing job allocation and avoiding network sharing have been proved to be effective for alleviating network contension and performance degradation. In this work, we propose a new job communication pattern aware allocation strategy which serve as a critical module in our future HPC batch scheduler. The novelty of our new strategy is that it makes allocation decision with not only system topology and resource availability information, but also with job's communication pattern information. The information scheduler gets about job communication pattern can help it to decide whether the job’s performance relies heavily on the network or not, and what the potential inter/intra-job interference would be. Thus, the scheduler can pick the preferable resource allocation based on specific job's communication pattern and preserve the locality of allocation in a better way. Using traces collected from three typical parallel applications of DOE, we validate the effectiveness of our new desgin. The idea presented in this paper is widely applicable: HPC parallel application has specific dominant communication pattern, allocating resource with communication pattern awareness can preserve locality in a bettwer way on systems with differnt network topologies.

\end{abstract}

\IEEEpeerreviewmaketitle


\section{Introduction} 
\label{sec: intro}

The scale of supercomputer keeps growing from petascale to exascale, to accomodate the the demand of insatiable computing power from scientific research areas. Production systems contain hundreds of thousands of processors serve as irreplaceable research vehicle for scientific problems with increasing size and complexity. Supercomputers employed in a shared way to accomodate many jobs running concurrently\ref{fig: mira activity} , which will also improve system utilization. These jobs share the system infrastructure such as network, I/O bandwidth, which will inevitably cause contention over the shared resource. As supercomputer continues to evolve, these shared resources are tend to become the bottleneck for performance.

\begin{figure}[h!] 
  \centering
  \includegraphics[width=0.38\textwidth]{figs/mira}
   \caption{The activity of Mira, IBM BlueGene/Q system at Argonne National Lab. Many jobs runninng concurrently on the system. Different jobs represented by specific colors. }
   \label{fig: mira activity}
\end{figure}

One of the most prominant problem comes into researcher's attention is the network contention between concurrently running jobs. The network sharing between concurrently running jobs cause communication variability, which reslut in jobs running slower and longer waiting time for results. The delay of currently running jobs will increase the queueing time of the following submitted jobs, thus leads to low system throughput, low utilization and high energy cost. The network sharing can even cause interference between neighbouring jobs, abort the the running job unexpectedly, or even worse, wrong science results. 


This adverse effect of network sharing can be mitigated by providing the job with better node allocation. The batch scheduler is responsible for assigning job with the amount of node it required upon submission. Currently two scheduling strategies are commonly used on torus-connected systems. One is so called partition based systems, where the scheduler assigns each user job a compact and contiguous set of computing nodes. IBM Blue Gene series systems fall into this category. This strategy is in favor of application’s performance by providing it with isolated partition and exclusive network connections, thus reducing network contention caused by concurrently running jobs sharing network bandwidth. However, this strategy can cause internal fragmentation (when more nodes are allocated to a job than it requests) and external fragmentation(when sufficient nodes are available for a request, but they can not be allocated contiguously), therefore leading to poor system performance (e.g., low system utilization and long waiting time for job). The other is non-contiguous allocation system, where free nodes are assigned to user job no matter whether they are contiguous or not. Cray XT/XE series systems fall into this category. Non-contiguous allocation eliminates internal and external fragmentation as seen in partition-based systems, thereby leading to high system utilization. Nevertheless, it introduces other problems such as scattering application processes all over the system and causing both inter-job contention and intra-job contention. The non-contiguous node allocation can make inter-process communication less efficient and cause network contention among concurrently running jobs, thereby resulting in poor job performance especially for those communication-intensive jobs.


Partition-based allocation achieves good job performance by sacrificing system performance (e.g., poor system utiliza- tion), whereas non-contiguous allocation can result in better system performance but could severely degrade performance of user jobs (e.g, prolonged wait-time and run-time). Supercomputers keep growing in size, its network become more sophisticated then ever before and applications also increase its complexity. How to provide application with better allocation that can balance job performance with system performance become a prominant problem need a solution before HPC enters era of exasacle computing. In this work, we propose a guideline for future batch scheduling system, a smart way to make allocation with the awareness of job's communication pattern.

Most existing batch schedulers take so few initiative because it been hold under the obscurantism policy. Whether it use partition-based or non-contigous policy, means it chooses the benefit of either system or job over the other. Some propose compromised policies like buddy system\cite{buddy system paper} still make the allocation for job blindly without knowing job's communication information. 

Although HPC applications complexity continue to grow, people could get more knowledge about the applications communication information with the help from various parallel application profiling tools, such as TAU\cite{tau}, MPIP\cite{mpip} and DUMPI\cite{sst}. Job's communication trace and performance analysis can be get by using such tool at runtime. 

The off-line study of job's communication traces could be helpful to develop better allocation policies for batch scheduler to make on-line scheduling/allocating decisions. This is because the communication pattern of parallel applications on HPC system can be summarized into a couple of categories\cite{hpdc2015-ornl}. The deep understanding of one application can be helpful to all other applications that conforms to the same communication pattern. Job's communication traces study would be extreme helpful to leadership computing facility too. The set of applications running on those leadership computing facilities is usually stable, and the workloads from those HPC systems is usually has great repetitiveness. We analyzed the workload on Mira, an IBM Blue/Gene Q system at Argonne Leadership Computing Facility, found that most jobs require 2k nodes and about 14 particular jobs consist of the majority its workload\ref{fig: repetitiveness of Mira}. In other word, Mira accommodates mostly some particular 14 jobs and most jobs have the same size.


In this work, we present a guideline for future batch scheduler to make allocation with awareness about job's communication pattern. Rather than choose blindly between partition-base or non-contigous allocation, the new allocation mechanism make allocation based on job's communication pattern. With detail analysis about application's communication pattern, we can get a clear view of its communication topology graph, data transfer between processes, and decide which processes are tightly connected to which and conduct intensive communication. The "local community" consists of such tightly coupled processes shuld get compact alloction. Then, the allocation mechanism should make allocations that only needs to guarantee the locality of the subset of the nodes where those "local community" should reside on.

We first pick three signature applications from DOE Design Forward Project\cite{design forward webpage} as examples to conduct deep study about their communication patterns. We identify the "local community" in these applications and assign them allocation accrodingly. The advantage of making allocation is obvious. First, compared with partition-based policy, our communication pattern aware policy is more flexible, it doesn't require the system to provide a big partition that can accomodate the whole application, just small set of compact nodes enough for all the "local community" in the application. On the other hand, our communication pattern aware allocation policy won't destroy the locality of application's communication pattern. On the contrary, it provides small compact node sest for the the "local communities" in the application will preserve the communication locality in the maximum extent.

We use a sophisticated simulation toolkit named CODES, Co-Design of Multi-layer Exascale Storage Architecture, from Argonne National Lab as research vehicle to evaluate the performance of our communication pattern aware allocation policy on torus network. CODES enables the exploration of simulating different HPC networks with high fidelity\cite{Jason-2011}. We extend CODES with a Job Mapping API that support different allocation policies on torus network.

We also conduct a case study with the results we got from communication pattern aware allocation policy evaluation with real trace from production system.\textcolor{red}{Not Sure About this part!!!}

The rest of this paper orgainzed as follows. Section\ref{sec:application study} gives a detailed study of the three applications from DOE Desgin Forward Project. Section\ref{sec:simulation} talks about the simulation platform and Section




\section{Application Study}
\label{sec:application study}

A parallel application usuall conforms to a combination of several basic communication patterns. At its different execution phases, the application's communication behavior may follow different basic patterns repectively. When we look into the data flow during the application execution, most parallel application start with broadcast operation to distribute the data from root process to other processes, followed by a series of computation and communication that comforms to certain pattern, which is usually the dominant part of application's execution. And before the application come to completion, all the working process will return their results to the root dirctly or hierarchically. In this work, we focus on the dominant communication pattern because that's where the application spend most of its running time. We believe making optimized allocation regarding application's dominant communication pattern would greatly benefit both the application and the system. In this section, we analysis two applications whose dominant communication patterns are most prevalent in scientific computation.

There are many profiling tools available to capture the communication behavior from parallel applications, such as Tuning and Analysis Utilities(TAU)\cite{tau}, mpiP\cite{mpip}. They can help analysis parallel application, providing information like the percentage of different MPI operations of the applications, communication topology, the amount data transferred between processes.

In this work, we provide detailed analysis about the communication pattern of two DOE MiniApps. MiniApps are reduced proxy applications that encapsulate the salient performance of larger full size applications\cite{miniapp}. Since we only focus on the dominant communication pattern of parallel applications, these MiniApps are perfect candidates for analysis the application's performance on different allocations or on new network architectures. We choose three MiniApps, namely, Algebraic Multigrid Solver(AMG), Geometric MultiGrid(MultiGrid) and CrystalRouter. Their communication patterns can be representative for most MiniApps listed on the DOE Design Forward Webpage\cite{design forward webpage}. The communication pattern figures we present here are generated with the IPM\cite{ipm} data from \cite{design forward webpage}.


\subsection{AMG}
\label{sec:amg}
AMG is the MiniApp of Algebraic Multigrid Solver, which is a parallel algebraic multigrid solver for linear systems arising from problems on unstructured mesh physics packages. It has been derived directly from the BoomerAMG solver that is being developed in the Center for Applied Scientific Computing (CASC) at LLNL\cite{amg}. The dominant communication pattern is this regional communication with decreasing message size for different parts of the multigrid v-cycle.

\begin{figure}[h!] 
  \centering
  \includegraphics[width=0.38\textwidth]{figs/appstudy/amg/amg_ct}
   \caption{Rank-Rank communication topology of AMG with 27 mpi ranks. This figure is redrawed by collected data from\cite{design forward webpage}.   }
   \label{fig: amg communication topology}
\end{figure}

\begin{figure}[h!] 
  \centering
  \includegraphics[width=0.38\textwidth]{figs/appstudy/amg/amg_data_transfer}
   \caption{The amount of data each rank transferred in AMG. }
   \label{fig: amg data trans}
\end{figure}

Figure \ref{fig: amg communication topology} shows the communication topology of a small scale AMG with 27 MPI Ranks. Here we prefer to show the communication topology with small scale version of the MiniApp because the dominant communication pattern of the application doesn't change with its scale. We can make the observation from the upper-left of the figure that each rank in AMG has intense communication between 3 other ranks, such as rank 0 between rank 1, 3 and 9, rank 1 between 2, 4 and 10, etc. And this relation is also symmetric along the main diagnal. The dominant communication pattern is quite obvious when we identify this relation between ranks.

Figure \ref{fig: amg data trans} shows the amount of data each rank transferred in AMG. The blue line shows that total data amount transferred from each rank, while the red shows the amount of received data and green shows the amount of send data. The first thing we can spot from the figure is that for each rank, the amount of data sent and received are basically the same, which also well explain the symmetry of the application's dominant communication pattern. And the second thing we can find about AMG is that the data transfer amount also has a symmetric pattern. Rank 0 has the same amout of data transfer as Rank 26, Rank 1 as of Rank 25, etc.


\subsection{Crystal Router}
\label{sec:crystalrouter}

The second MiniApp we have is Crystal Router, which is the extracted communication kernel of the full application Nek5000. Nek5000\cite{crystalrouter} is a spectral element CFD application developed at Argonne National Laboratory. It features spectral element multigrid solvers coupled with a highly scalable, parallel coarse-grid solver that widely used for projects including ocean current modeling, thermal hydraulics of reactor cores, and spatiotemporal chaos. The MiniApp of Nek5000, Crystal Router demonstrates the "many-to-many" communication pattern through scalable multi-stage communication process.

\begin{figure}[h!] 
  \centering
  \includegraphics[width=0.38\textwidth]{figs/appstudy/cr/cr10_ct}
   \caption{Rank-Rank communication topology of CrystalRouter with 10 mpi ranks. This figure is redrawed by collected data from\cite{design forward webpage}. }
   \label{fig: crystalrouter10 communication topology}
\end{figure}

\begin{figure}[h!] 
  \centering
  \includegraphics[width=0.38\textwidth]{figs/appstudy/cr/cr100_ct}
   \caption{Rank-Rank communication topology of CrystalRouter with 100 mpi ranks. This figure is redrawed by collected data from\cite{design forward webpage}. }
   \label{fig: crystalrouter100 communication topology}
\end{figure}

\begin{figure}[h!] 
  \centering
  \includegraphics[width=0.38\textwidth]{figs/appstudy/cr/cr_data_transfer}
   \caption{The amount of data each rank transferred in CrystalRouter. }
   \label{fig: cr data trans}
\end{figure}

Here, we provide the communication topology graph of Crystal Router with 10-rank and 100-rank respectively. As we can see from figure \ref{fig: crystalrouter10 communication topology}, every 5-rank is a compact community, where data transfer is very intensive. Beside the "5-rank community", there is "pair-wise" global data transfer in Crystal Router. Obviously, there is data transfer alone the diagnal. When the scale of Crystal Router increase to 100, the "5-rank community" is still the dominant pattern in a multi-stage manner. The same pattern can be observed from the global data transfer. Our observation about the dominant communication pattern of Crystal Router is the multi-stage "5-rank community" and "pair-wise" global data transfer happen recursively as the scale of the application increase.

Figure \ref{fig: cr data trans} shows the amount of data each rank transferred in Crystal Router. The locality pattern of "5-rank community" can still be spotted in this figure. The spiky shape occurs in the frequency of every 5 rank. Unlike AMG, the there is variance between the amount of data send and received for each rank. This is due to the specific feature of Crystal Router's communication pattern.


\subsection{MultiGrid}
\label{sec:multigrid}

Differe from AMG, MultiGrid is geometric multigrid v-cycle from production elliptic solver BoxLib, a software framework for massively parallel block-structured adaptive mesh refinement (AMR) codes. MultiGrid conforms to 3D lattice communication pattern with decreasing message size and collectives for different parts of the multigrid v-cycle. It is widely used for structured grid physics packages.

\begin{figure}[h!] 
  \centering
  \includegraphics[width=0.38\textwidth]{figs/appstudy/mg/mg125_ct}
   \caption{Rank-Rank communication topology of MultiGrid with 125 mpi ranks. This figure is redrawed by collected data from\cite{design forward webpage}. }
   \label{fig: multigrid125 communication topology}
\end{figure}

\begin{figure}[h!] 
  \centering
  \includegraphics[width=0.38\textwidth]{figs/appstudy/mg/mg_data_transfer}
   \caption{The amount of data each rank transferred in MultiGrid. }
   \label{fig: cr data trans}
\end{figure}


\section{Research Vehicle}
\label{sec:simulation}

A simulation toolkit named CODES (Co-Design of Multi-layer Exascale Storage Architecture) from Argonne National Lab enables the exploration of simulating different HPC networks with high fidelity\cite{Jason-2011}\cite{mubarak-sc2012}. CODES is built on top of Rensselaer Optimistic Simulation System (ROSS) parallel discrete-event simulator, which is capable of processing billions of events per second on leadership-class supercomputers\cite{ross}. CODES support both torus and dragonfly network with high fidelity flit-level simulation. In this work, we only use torus network to show the impact of node allocation to job with different dominant communication patterns. CODES has this network workload component that capable of conducting trace-driven simulation. It can take real MPI application trace generated by SST DUMPI\cite{sst} to drive CODES network models. 


Torus networks have been extensively used in the current generation of supercomputers because of their linear scaling on per-node cost and competitive communication performance. The topology of torus network is k-ary n-cube, with $k^n$ nodes in total arranged in an n-dimensional grid having k nodes in each dimension. Each node has 2$\times$ n direct linked neighbor nodes. 

For a job requires $n$ nodes submitted to non-partition based 3D torus-connected system, it gets an allocation of various possible shapes, which could be 3D balanced-cube, 3D unbalanced-cube, 2D mesh, or even 1D list. Figure \ref{fig: allocation} shows some possible allocations for a 64-node job. 3D balanced-cube allocation (in Figure \ref{fig: allocation} as red) is $\{4,4,4\}$, 3D unbalanced-cube allocation is $\{8,4,2\}$(in Figure \ref{fig: allocation} as green), 2D mesh allocation is $\{8,8,1\}$ (in Figure \ref{fig: allocation} as blue).

\begin{figure}[h!] 
  \centering
  \includegraphics[width=0.48\textwidth]{Pic/allocationshape/allocation}
  \caption{64-node job with different allocation shapes. Red, Green and Blue represents 3D balanced, 3D unbalanced and 2D allocation respectively. }
  \label{fig: allocation}
\end{figure}

We implemented a Job Mapping API for CODES, which can provide flexible job allocation strategies. The new API can help us to explore the impact of different allocation strategies to job with specific communication pattern. We have choosen two publicly available HPC network traces provided by the Design Forward Program\cite{design forward webpage}. The detailed analysis of these two traces are present in Section\ref{sec:amg} and \ref{sec:crystalrouter}.


\subsection{Allocation Simulation}
\label{sec:alloc sim}

In this section, we analysis the impact of different allocation strategies on two real applications, AMG and Grystal Router. The torus network performance is determined by its dimensionality and link bandwidth. We evaluate the data transfer time of AMG and Crystal Router on torus network with different dimensionality and link bandwidth configurations in section \ref{sec: dimensionality study}. Then, we will show applications performance on 3D torus network with different allocation strategies in section \ref{sec: alloc strategy study}.

\subsection{Torus Dimensionality}
\label{sec: dimensionality study}

As the increase of the dimensionality of torus netowrk, so does the number of links connected with each node. The increased aggregated bandwidth of each node will definitely reduce the data transfer time of each rank in the application. Figure xxx shows the performance of the AMG and CrystalRouter on a 2K node torus network model with a 3D torus (16x16x8), a 5D torus (8x4x4x4x4), and a 7D torus (4x4x4x4x2x2x2). The bandwidth between nodes is 2GiB/s in one direction, thus, the aggregated bandwidth is 12GiB/s per node in 3D torus, 20GiB/s per node in 5D torus, and 28GiB/s per node in 7D torus. As we can see from the figure, communication time of both applications decrease as the dimensionality of the network increase. The aggregated bandwidth of each node can accelerate the transfer of data. This can justify the fidelity and consistency of the torus network model.
\textcolor{green}{cr1000 on 7D result is missing in Fig\ref{fig: dimenstudy-avgtime} and \ref{fig: dimenstudy-maxtime} , will be added soon.}

\begin{figure}[h!] 
  \centering
  \includegraphics[width=0.38\textwidth]{figs/dimenstudy/maxtime}
   \caption{The Percentage of Maximum Time spent to send data over 3D, 5D and 7D torus networks by AMG1728, CrystalRouter1000, MultiGrid1000.}
   \label{fig: dimenstudy-maxtime}
\end{figure}

\begin{figure}[h!] 
  \centering
  \includegraphics[width=0.38\textwidth]{figs/dimenstudy/avgtime}
   \caption{The Percentage of Average Time spent to send data over 3D, 5D and 7D torus networks by AMG1728, CrystalRouter1000, MultiGrid1000.}
   \label{fig: dimenstudy-avgtime}
\end{figure}

\textcolor{blue}{sensitivity study of 5D torus with 2GB,4GB,8GB will be added here. }


\textcolor{red}{We can also provide Per-Rank time spent for data transfer. not sure if necessay.}


\section{Interference Analysis}
In this section, we will explore the performance of three applications on different allocations in a 3D torus network. First, we analysis the impact from different allocation shape, then we will present the results by using our communication aware allocation policy.  

\subsection{Different Allocation Shape}
As we shown in Fig\ref{fig: allocation}, there could be three differet allocation shape on 3D torus network, namely 3D balanced, 3D un-balanced, and 2D. In this section, we will show the performance of three applications on these three different shape allocations.

There are lots of research work try to design fancy allocation algorithms to provide application with most compact allocation, like using space filling curve (SFC) to index high dimentional cube into one dimensional list. Allocation by cutting chunks of nodes from this one dimensional list will guarantee cubical allocation. However, try to provide cubical allocation without considering application's communication pattern could cause backfire.

\textcolor{red}{Detail explanation will be add for Fig\ref{fig: shapestudy-avgtime}-Fig\ref{fig: shapestudy-mg125}.}

\begin{figure}[h!] 
  \centering
  \includegraphics[width=0.38\textwidth]{figs/shapestudy/avgtime}
   \caption{The Percentage of Average Time spent to send data by AMG216, CrystalRouter100, MultiGrid125 with 2D, 3D-unbalanced, 3D-balanced allocation.}
   \label{fig: shapestudy-avgtime}
\end{figure}

\begin{figure}[h!] 
  \centering
  \includegraphics[width=0.38\textwidth]{figs/shapestudy/maxtime}
   \caption{The Percentage of Max Time spent to send data by AMG216, CrystalRouter100, MultiGrid125 with 2D, 3D-unbalanced, 3D-balanced allocation.}
   \label{fig: shapestudy-maxtime}
\end{figure}

\begin{figure}[h!] 
  \centering
  \includegraphics[width=0.38\textwidth]{figs/shapestudy/amg216_rank_msgtime}
   \caption{(AMG216) Time spent by each rank to send data on 2D, 3D-unbalanced, 3D-balanced allocation. }
   \label{fig: shapestudy-amg216}
\end{figure}

\begin{figure}[h!] 
  \centering
  \includegraphics[width=0.38\textwidth]{figs/shapestudy/cr100_rank_msgtime}
   \caption{(CrystalRouter100) Time spent by each rank to send data on on 2D, 3D-unbalanced, 3D-balanced allocation.}
   \label{fig: shapestudy-cr100}
\end{figure}

\begin{figure}[h!] 
  \centering
  \includegraphics[width=0.38\textwidth]{figs/shapestudy/mg125_rank_msgtime}
   \caption{(MultiGrid125) Time spent by each rank to send data on 2D, 3D-unbalanced, 3D-balanced allocation.}
   \label{fig: shapestudy-mg125}
\end{figure}

\subsection{Allocation Strategies Study}
\label{sec: alloc strategy study}

We proposed two allocation strategies, Compact and Chunk to study the impact of different allocation the application's performance. The Compact allocation always provides application with a cubical allocation on 3D torus, regardless application's communication pattern. This cubical allocation will keep the locality of the application, all the processes reside on it will have the shortest pair-wise distance. 

The Chunk allocation simulation the scenario of non-contiguous allocation strategy employed in production system like Cray XT machines. The traditional non-contiguous allocation simply picks available nodes in spite of their locations and assign them to application. The motivation of non-contiguous allocation act like this is to maximize system utilization, while being blind to application's communication pattern would greatly prolong application's communication time. Although Chunk allocation provides application with non-contigous allocation too, it assign the chunk of nodes in consideration of application's communication pattern. For example, we know the dominant communication pattern of CrystalRouter in section \ref{sec:crystalrouter}, every 5 rank form a community, where intensive communication happens between them. With such knowledge knowledge about application's communication pattern, the Chunk allocation will provide CrystalRouter with allocation that consists of small chunks, the size of which will be at least 5 nodes. 

Figure xxx shows the performance of AMG, CrystalRouter and MultiGrid on 3D torus with Compact and Chunk allocation. 


\textcolor{red}{The results and figures for this part is not ready yet!!!}




\section{Related Work}
\label{sec:related_work}
Batch scheduling on HPC system has been a hot topic getting constant discussion since the 1990's.
Feitelson et al proposed the most widely used job scheduling policy which is First Come, First Serve (FCFS) combined with 
EASY backfilling\cite{feit}. FCFS/EASY backfilling policy greatly improves HPC system's utilization and cuts the wait time 
of first job in the waiting queue. Since then, many studies seek to refine this classic scheduling paradigm. Tang et al. 
tried to improve the accuracy of user's estimated job runtime and walltime so as to enhance the efficiency of backfilling and  minimize the system fragmentation\cite{wei-ipdps2010} \cite{wei-jpdc2013} \cite{wei-ipdps2011}. % And there are some other variation of FCFS/EASY backfilling proposed to optimize system performance in terms of power consumption and energy cost \cite{zhou}\cite{yang-sc13}.


Only with the information about job size and estimated run time seems to be insufficient for batch scheduler do make the 
optimal scheduling/allocaiton decision. Some work proposed that the batch scheduler should be aware of some information 
about system network information in order to fully utilize system resources.Leung et al. presented allocation strategies 
based on space filling curves and one dimensional packing \cite{leung}. The problem with using space filling curve is that  
it can only be applied to system with the scale of $2^n$ nodes in each dimension, which is a ideal case that not existing 
in morden HPC systems. Albing et al. conducted study about the allocation strategies that the Cray
Application Level Placement Scheduler (ALPS) used \cite{carl-cug}. The job allocation in Cray Linux Environment (CLE) 
operating system is managed by ALPS, which simply works off an ordered node list, however that is ordered. 

Based on their 
work, Yang et al. presented a window-based locality-aware job scheduling design for torus-connected system
\cite{yang-cluster14}. In this new scheduling framework, several jobs are taken into  consideration at the same time for job 
prioritizing and resource allocation. A list of slots is maintained to preserve node contiguity information for
resource allocation. Each job been assigned into one of these slot so that the locality can be mostly preserved.

Rosenthal et al. found there is limited benefit for many applications by
increasing network bandwidth \cite{rosenthal}. They employed a subset of the
CORAL mini-applications that represent U.S. Department of Energy workload and
leverage multirail networkings to evaluate the improvement of these applications
caused by increased network bandwidth. The applications that send mostly small
messages or larger messages asynchronously are not bandwidth bounded, hence,
benefit only slightly or not at all from increased bandwidth.

   

Hoefler et al. propose to use performance modeling techniques to analysis 
factors that impact the performance of parallel scientific applications 
\cite{hoefler-modeling}. However, as the scale of HPC continue grows, the 
interference of concurrently running jobs is getting worse, which is hard to be 
quantified by performance profiling tools.


Bogdan et al provide a set of guidelines how to configure a network with 
Dragonfly topology for workload with Nearest Neighbor communication 
pattern\cite{Bogdan-hpdc14}. They first derived a theoretical model of Nearest 
Neighbor communication performance on Dragonfly network that can predict network 
bottleneck. Then, they use simulation to validate the correctness of their 
theoretical model.

Dong et al describe IBM Blue Gene/Q's 5D torus interconnect
network\cite{Dong-SC11}. They developed simple benchmarks that conforms to four
different communication patterns, namely ping-pong, nearest neighbor, broadcast
and allredcue, to demonstrate the effectiveness of this highly parallel 5D torus
network.



\section{Conclusions}
\label{sec:conclusion}


\section*{Acknowledgment}
\label{sec: ack}
%We appreciate the valuable comments and suggestions from the anonymous reviewers. 
The work at Illinois Institute of Technology is supported in part by
US National Science Foundation grant CNS-1320125. The work at Argonne is
supported in part by the U.S. Department of Energy (DOE), Office of Science,
under Contract DE-AC02-06CH11357.



\begin{thebibliography}{10}
 %=====start of scheduling papers===== 
\bibitem{feit}
D.~Feitelson and A.~Weil.
\newblock Utilization and predictability in scheduling the {IBM} {SP2} with
  backfilling.
\newblock In {\em International Parallel and Distributed Processing Symposium}, 1998.

\bibitem{wei-ipdps2010}
W.~Tang, N.~Desai, D.~Buettner, and Z.~Lan.
\newblock Analyzing and adjusting user runtime estimates to improve job
  scheduling on the {Blue} {Gene/P}.
\newblock In {\em 2010 IEEE International Symposium on Parallel Distributed
  Processing}, 2010.

\bibitem{wei-ipdps2011}
W.~Tang, Z.~Lan, N.~Desai, D.~Buettner, and Y.~Yu.
\newblock Reducing fragmentation on torus-connected supercomputers.
\newblock In {\em 2011 IEEE International Symposium on Parallel Distributed
  Processing Symposium}, 2011.

\bibitem{wei-jpdc2013}
W.~Tang, N.~Desai, D.~Buettner, and Z.~Lan. 
\newblock Job Scheduling With Adjusted Runtime Estimates on Production Supercomputers.
\newblock Journal of Parallel and Distributed Computing (JPDC), 73(7):926-938, 2013.

\bibitem{zhou}
Z.~Zhou, Z.~Lan, W.~Tang, and N.~Desai.
\newblock Reducing energy costs for {IBM Blue} {Gene/P} via {Power-Aware} job
  scheduling.
\newblock In {\em 17th Workshop on Job Scheduling Strategies for Parallel
  Processing}, 2013.

\bibitem{yang-sc13}
X. ~Yang, Z.~Zhou, S.~Wallace Z.~Lan, W.~Tang, S. ~Coghlan and Mike. ~Papka.
\newblock Integrating Dynamic Pricing of Electricity into Energy Aware Scheduling for HPC Systems. 
\newblock In {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis(SC), 2013 International Conference for}
\newblock SC '13,
\newblock 2013,
\newblock Salt Lake City, Utah

\bibitem{Cqsim}
{\textcolor{red} {Cqsim: An event-driven simulator}}.
\newblock http://bluesky.cs.iit.edu/cqsim


%=====start of allocation aware papers===== 
\bibitem{leung}
Leung, V.J.; Arkin, E.M.; Bender, M.A.; Bunde, D.; Johnston, J.; 
Alok Lal; Mitchell, J.S.B.; Phillips, C.; Seiden, S.S., 
\newblock Processor allocation on Cplant: achieving general processor locality using one-dimensional allocation strategies
\newblock In {Proceedings of 2002 IEEE International Conference on Cluster Computing, 2002}.  pages 296--304, 2002

\bibitem{carl-cug}
Carl Albing and Mark Baker
\newblock ALPS,Topology, and Performance: A Comparison of Linear Orderings for 
Application Placement in a 3D torus.
\newblock Presented at the CUG 2010, Edinburgh, Scotland, UK, 2010.

\bibitem{yang-cluster14}
Xu Yang and Zhou Zhou and Wei Tang and Xingwu Zheng and Jia Wang and Zhiling Lan.
\newblock Cluster Computing (CLUSTER), 2014 IEEE International Conference on,
\newblock Balancing job performance with system performance via locality-aware scheduling on torus-connected systems
\newblock Sept, 2014,
\newblock pp.140-148

%+++++++++++++++++++++++
\bibitem{mpip}
\newblock mpiP: Lightweight, Scalable MPI Profiling.
\newblock http://mpip.sourceforge.net, 
\newblock 2013.

\bibitem{tau}
S. Shende and A. Malony. 
\newblock The TAU parallel performance system. 
\newblock International Journal of High Performance Computing Applications, 20(2):287–311, 2006.

\bibitem{scala}
X. Wu, F. Mueller, S. Pakin, 
\newblock "Automatic Generation of Executable Communication Specifications from Parallel Applications", 
\newblock In {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis(SC), 2011 International Conference for}
\newblock SC '11,
\newblock 2011,pages 12-21.

\bibitem{design forward webpage}
Department of Energy, “Characterization of the DOE Mini-apps.” [Online].
\newblock Available: http://portal.nersc.gov/project/CAL/trace.htm
\newblock (Accessed on: Sept 16th. 2015)

\bibitem{ipm}
David Skinner, Nicholas Wright, Karl Fuerlinger and Kathy Yelick
\newblock Integrated Performance Montoring(IPM)
\newblock Available http://ipm-hpc.sourceforge.net/overview.html
\newblock (Accessed on: Sept 16th. 2015)


\bibitem{miniapp}
Bronson Messer
\newblock "Using a Developing MiniApp to Compare Platform Characteristics on Cray Systems"
\newblock In Proceedings of Cray User Group 2012

\bibitem{amg}
V. E. Henson and U. M. Yang, 
\newblock "BoomerAMG: A Parallel Algebraic Multigrid Solver and Preconditioner",
\newblock Appl. Num. Math. 41 (2002), pp. 155-177. UCRL-JC-141495.

\bibitem{crystalrouter}
P. Fischer, J. Lottes, D. Pointer, and A. Siegel, 
\newblock “Petascale algorithms for reactor hydrodynamics,”
\newblock Journal of Physics: Conference Series, vol. 125, no. 1, p. 012076, 2008.

%+++++++++++++++++

%=====start of network papers===== 
\bibitem{rosenthal}
E. Rosenthal and E.A. Leon
\newblock Characterizing Application Sensitivity to Network Performance.
\newblock  In {Proceedings of SC14: International
Conference for High Performance Computing, Networking, Storage and Analysis},
2014

\bibitem{hoefler-modeling}
T. Hoefler and W. Gropp and M. Snir and W. Kramer
\newblock Performance Modeling for Systematic Performance Tuning,
\newblock 2011
\newblock Nov.
\newblock In {Proceedings of International Conference for High Performance Computing, Networking, Storage and Analysis (SC'11), SotP Session,
}

\bibitem{roth}
P. Roth and J. Meredith and J. Vetter,
\newblock "Automated Characterization of Parallel Application Communication Patterns",
In Proceedings of the 25rd International Symposium on High-performance Parallel and Distributed Computing
\newblock HPDC '15,
\newblock 2015,


%=====end of application/mapping papers===== 
\bibitem{Pjesivac}
Pje\v{s}ivac-Grbovi\'{c}, Jelena and Angskun, Thara and Bosilca, George and Fagg, Graham E. and Gabriel, Edgar and Dongarra, Jack J.
\newblock Performance Analysis of MPI Collective Operations,
\newblock Cluster Computing,
\newblock June 2007,
\newblock volume 10,
\newblock numpages 17,
 

\bibitem{thakur}
Thakur, Rajeev and Gropp, William D.
\newblock Improving the performance of collective operations in MPICH,
\newblock Recent Advances in Parallel Virtual Machine and Message Passing Interface,
\newblock year 2003,
\newblock Springer

\bibitem{abhinav-ipdps15}
Abhinav Bhatele, Andrew R. Titus, Jayaraman J. Thiagarajan, Nikhil Jain, Todd Gamblin,
Peer-Timo Bremer, Martin Schulz† and Laxmikant V. Kale,
\newblock "Identifying the Culprits behind Network Congestion",
\newblock In {\em 2015 IEEE International Symposium on Parallel Distributed Processing Symposium}, 2015.

\bibitem{langer}
S. Langer, A. Bhatele, and C. H. Still,
\newblock “pF3D simulations of laser-plasma interactions in National Ignition Facility experiments,” 
\newblock Computing in Science and Engineering, vol. 99, Aug. 2014, lLNL- JRNL-648736. 
\newblock [Online]. Available: http://doi.ieeecomputersociety.org/ 10.1109/MCSE.2014.79

%\bibitem{abhatele-ipdps08}
%A. Bhatele, L.V.  Kale
%\newblock Application-specific topology-aware mapping for three dimensional topologies.
%\newblock In {\em Proceedings of Workshop on Large-Scale Parallel Processing (held as part of IPDPS' 08)} 2008
%
%\bibitem{abhinav-sc13} 
%Bhatele, Abhinav and Mohror, Kathryn and Langer, Steven
%H. and Isaacs, Katherine E. 
%\newblock There Goes the Neighborhood: Performance Degradation Due to Nearby Jobs 
%\newblock In {Proceedings of SC13: International
%Conference for High Performance Computing, Networking, Storage and Analysis},
%2013

\bibitem{hpdc14}
Prisacari, Bogdan and Rodriguez, German and Heidelberger, Philip and Chen, Dong and Minkenberg, Cyriel and Hoefler, Torsten
\newblock Efficient Task Placement and Routing of Nearest Neighbor Exchanges in Dragonfly Networks
\newblock In Proceedings of the 23rd International Symposium on High-performance Parallel and Distributed Computing
\newblock HPDC '14,
\newblock 2014,
\newblock Vancouver, BC, Canada

\bibitem{jingjin}
Wu, Jingjin and Lan, Zhiling and Xiong, Xuanxing and Gnedin, Nickolay Y. and Kravtsov, Andrey V.
\newblock Hierarchical Task Mapping of Cell-based AMR Cosmology Simulations
\newblock Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis
\newblock SC '12,
\newblock 2012,
\newblock Salt Lake City, Utah

\bibitem{zhou-ipdps}
Z. Zhou, X. Yang, Z. Lan, P. Rich, W. Tang, V. Morozov, and N. Desai.
\newblock Improving Batch Scheduling on Blue Gene/Q by Relaxing 5D Torus Network Allocation Constraints,
\newblock Proceedings of 29th IEEE International Parallel \& Distributed Processing 
Symposium,
\newblock IPDPS'15, 2015.
\newblock Hyderabad, INDIA

\bibitem{Bogdan-hpdc14}
Prisacari, Bogdan and Rodriguez, German and Heidelberger, Philip and Chen, Dong and Minkenberg, Cyriel and Hoefler, Torsten
\newblock Efficient Task Placement and Routing of Nearest Neighbor Exchanges in Dragonfly Networks,
\newblock Proceedings of the 23rd International Symposium on High-performance Parallel and Distributed Computing,
\newblock HPDC '14,
\newblock 2014,
\newblock  Vancouver, BC, Canada,


\bibitem{hpdc2015-ornl}
Philip. C.R, Jeremy S. M, etc
\newblock "Automated Characterization of Parallel Application Communication Patterns"
\newblock HPDC'15

\bibitem{Dong-SC11}
Dong Chen; Eisley, N.A.; Heidelberger, P.; Senger, R.M.; Sugawara, Y.; Kumar,
S.; Salapura, V.; Satterfield, D.L.; Steinmacher-Burow, B.; Parker, J.J., 
\newblock "The IBM Blue Gene/Q interconnection network and message unit," 
\newblock High Performance Computing, Networking, Storage and Analysis (SC), 2011 International 
Conference,
\newblock pp.1,10, 
\newblock 12-18 Nov. 2011

\bibitem{Jason-2011}
Jason Cope, Ning Liu, Sam Lang, Phil Carns, Chris Carothers, Robert Ross.
\newblock Proceedings of the Workshop on Emerging Supercomputing Technologies 2011,
\newblock 2011


%===simulation papers =====

\bibitem{ross}
P. D. Barnes, C. D. Carothers, D. R. Jefferson, and J. M. LaPre, 
\newblock “Warp speed: executing time warp on 1,966,080 cores,”
\newblock in Proc. of the 2013 ACM SIGSIM Conf. on Principles of Advanced Discrete Simulation (PADS),
\newblock May 2013, pp. 327–336.

\bibitem{mubarak-sc2012}
Mubarak, M.; Carothers, C.D.; Ross, R.; Carns, P.,
\newblock "Modeling a Million-Node Dragonfly Network Using Massively Parallel Discrete-Event Simulation," 
\newblock High Performance Computing, Networking, Storage and Analysis (SCC), 
\newblock 2012 SC Companion: 
\newblock pp.366,376, 10-16 Nov. 2012

\bibitem{sst}
Sandia National Labs, “SST DUMPI trace library.” [Online].
\newblock Available: http://sst.sandia.gov/using dumpi.html 
\newblock (Accessed on: Sept 3rd, 2015)


\end{thebibliography}

\end{document}


